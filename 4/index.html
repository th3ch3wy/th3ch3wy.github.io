<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CS 180/280A Project 4: Neural Radiance Field</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
      background: #f7f7fb;
      color: #222;
    }
    header {
      background: #003262;
      color: #fff;
      padding: 1.5rem 1rem;
    }
    header h1 {
      margin: 0;
      font-size: 1.75rem;
    }
    header p {
      margin: 0.25rem 0;
    }
    .container {
      max-width: 1000px;
      margin: 0 auto;
      padding: 1.5rem 1rem 3rem;
    }
    nav {
      margin: 1rem 0 2rem;
      background: #ffffff;
      border-radius: 0.75rem;
      padding: 0.75rem 1rem;
      box-shadow: 0 2px 5px rgba(0,0,0,0.06);
      font-size: 0.95rem;
    }
    nav a {
      margin-right: 1rem;
      text-decoration: none;
      color: #003262;
      font-weight: 500;
    }
    nav a:hover {
      text-decoration: underline;
    }
    section {
      margin-bottom: 2.5rem;
      background: #ffffff;
      border-radius: 0.75rem;
      padding: 1.25rem 1.5rem 1.5rem;
      box-shadow: 0 2px 5px rgba(0,0,0,0.06);
    }
    section h2 {
      margin-top: 0;
      border-bottom: 2px solid #eee;
      padding-bottom: 0.4rem;
    }
    section h3 {
      margin-top: 1.5rem;
    }
    .image-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(220px, 1fr));
      gap: 1rem;
      margin-top: 0.75rem;
    }
    figure {
      margin: 0;
      background: #fafafa;
      border-radius: 0.5rem;
      padding: 0.5rem;
      border: 1px solid #eee;
    }
    figure img, figure video {
      width: 100%;
      display: block;
      border-radius: 0.35rem;
    }
    figcaption {
      font-size: 0.85rem;
      color: #555;
      margin-top: 0.4rem;
    }
    ul {
      padding-left: 1.2rem;
    }
    code {
      background: #f0f0f5;
      padding: 0.1rem 0.25rem;
      border-radius: 0.25rem;
      font-size: 0.9em;
    }
    .small-note {
      font-size: 0.85rem;
      color: #666;
    }
  </style>
</head>
<body>
  <header>
    <h1>CS 180/280A Project 4: Neural Radiance Field</h1>
  </header>

  <div class="container">
    <nav>
      <a href="#part0">Part 0</a>
      <a href="#part1">Part 1</a>
      <a href="#part2">Part 2</a>
      <a href="#part26">Part 2.6</a>
    </nav>

    <!-- ===================== PART 0 ===================== -->
<section id="part0">
  <h2>Part 0: Camera Calibration and 3D Scanning</h2>

  <h3>0.1–0.3: Calibration, Pose Estimation, and Visualization</h3>
  <p>
    I first calibrated my camera using ArUco tags and OpenCV’s
    <code>cv2.calibrateCamera</code>, then used <code>cv2.solvePnP</code> to estimate camera
    poses for the object images. I used the provided <code>viser</code> code to visualize
    camera frustums.
  </p>

  <h4>Camera Frustums in Viser (Deliverable: 2 screenshots)</h4>

  <!-- Simple vertical layout, no grid -->
  <figure style="text-align:center; margin-bottom: 2rem;">
    <img src="source/calibration_sample.JPG" 
         style="width: 50%; border-radius: 10px;">
    <figcaption>Sample calibration image with ArUco board.</figcaption>
  </figure>

  <figure style="text-align:center; margin-bottom: 2rem;">
    <img src="source/object_sample.JPG"
         style="width: 50%; border-radius: 10px;">
    <figcaption>Sample object capture with ArUco tag.</figcaption>
  </figure>

  <!-- Larger Viser images -->
  <figure style="text-align:center; margin-bottom: 2rem;">
    <img src="source/viser0.3a.png"
         style="width: 80%; max-width: 900px; border-radius: 10px;">
  </figure>

  <figure style="text-align:center; margin-bottom: 2rem;">
    <img src="source/viser0.3b.png"
         style="width: 80%; max-width: 900px; border-radius: 10px;">
  </figure>

  <h3>0.4: Undistortion and Dataset Creation</h3>
  <p>
    Using the recovered intrinsics and distortion coefficients, I undistorted all images with
    <code>cv2.undistort</code>, optionally cropping using
    <code>cv2.getOptimalNewCameraMatrix</code> and updating the principal point.
    I then split the images into train/val/test sets and saved them in the required
    <code>.npz</code> format: <code>images_train</code>, <code>images_val</code>,
    <code>c2ws_train</code>, <code>c2ws_val</code>, <code>c2ws_test</code>, and <code>focal</code>.
  </p>
</section>

    <!-- ===================== PART 1 ===================== -->
    <section id="part1">
      <h2>Part 1: Fit a Neural Field to a 2D Image</h2>

      <h3>Model Architecture (Deliverable)</h3>
      <p>
        I implemented an MLP with sinusoidal positional encoding that maps normalized 2D
        pixel coordinates <code>(u, v)</code> to RGB values in <code>[0,1]</code>.
      </p>
      <ul>
        <li><strong>model = NeuralField2D(in_dim=in_dim, hidden_dim=256, num_layers=4).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

N = coords.shape[0]
batch_size = 8192
num_iters = 5000</li>
      </ul>

      <h3>Training Progression (Deliverable)</h3>
      <p>
        Below are training snapshots for the provided test image and my own image at different
        iterations.
      </p>

      <div class="image-grid">
        <figure>
          <img src="source/fox_iters.png" alt="Provided test image reconstruction final">
        </figure>
        <figure>
          <img src="source/fox_plot.png">
        </figure>
      </div>

      <h4>My Own Image</h4>
      <div class="image-grid">
        <figure>
          <img src="source/cat_iters.png">
        </figure>
        <figure>
          <img src="source/cat_plot.png">
        </figure>
      </div>

      <h3>Effect of Positional Encoding Frequency and Width (Deliverable)</h3>
      <p>
        Here I show a 2x2 grid of final reconstructions with different combinations of
        PE frequency <code>L</code> and network width.
      </p>
      <div class="image-grid">
        <figure>
          <img src="source/cat2_iters.png" alt="Default frequency (10), Default width (256)">
          <figcaption>Default frequency (10), Default width (256)</figcaption>

        </figure>
        
      </div>
      <div class="image-grid">
        
        <figure>
          <img src="source/cat3_iters.png" alt="Low frequency (2)">
          <figcaption>Low frequency (2), Default width (256)</figcaption>

        </figure>
      
      </div>
      <div class="image-grid">

        <figure>
          <img src="source/cat4_iters.png" alt="low width (64)">
          <figcaption>Default frequency (10), Low width (64)</figcaption>

        </figure>
      </div>
    </section>


    <!-- ===================== PART 2 (LEGO) ===================== -->
    <section id="part2">
      <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>

      <h3>Implementation Overview (Deliverable)</h3>
      <p>
        In this section, I implemented the components required for NeRF on the Lego dataset:
      </p>
      <ul>
        <li><strong>Coordinate transforms:</strong> <code>transform(c2w, x_c)</code>, <code>pixel_to_camera</code>, <code>pixel_to_ray</code>.</li>
        <li><strong>Ray sampling:</strong> Random ray sampling across images, and point sampling along rays with perturbation.</li>
        <li><strong>NeRF MLP:</strong> A deeper network taking PE-encoded 3D positions and view directions, outputting density and RGB.</li>
        <li><strong>Volume rendering:</strong> Implemented the discrete volume rendering equation in PyTorch, matching the provided assert.</li>
      </ul>

      <h3>Visualization of Cameras, Rays, and Samples (Deliverable)</h3>
      <p>
        Using <code>viser</code>, I visualized the training cameras together with a subset of rays
        and sampled points.
      </p>
      <div class="image-grid">
        <figure>
          <img src="source/viser2.png" alt="Viser plot of cameras, rays, and sampled points">
        </figure>
      </div>

      <h3>Training Progression on Lego (Deliverable)</h3>
      <p>
        Below are predicted renderings from a fixed validation or novel-view camera at different
        iterations of training.
      </p>
      <div class="image-grid">
        <figure>
          <img src="source/lego_iters.png" alt="Lego novel view at iteration 0">
        </figure>
        <figure>
          <img src="source/lego_plot.png" alt="Lego novel view at iteration 500">
        </figure>
      </div>

      <h3>PSNR Curve on Validation Set (Deliverable)</h3>
      <figure>
        <img src="images/part2_psnr_val_curve.png" alt="Validation PSNR over training for Lego">
        <figcaption>PSNR on the 6 validation images over training iterations.</figcaption>
      </figure>

      <h3>Spherical Rendering of Lego (Deliverable)</h3>
      <p>
        Using the provided <code>c2ws_test</code> extrinsics, I rendered a spherical “orbit”
        video around the Lego object.
      </p>
      <figure style="text-align:center; margin:20px 0;">
  <img src="source/lego_orbit.gif">
</figure>

    </section>

    <!-- ===================== PART 2.6 (OWN DATA) ===================== -->
    <section id="part26">
      <h2>Part 2.6: Training with My Own Data</h2>

      <h3>Object and Data Collection</h3>
      <p>
        For my own NeRF, I captured an object of my choice (describe object here) together with
        a single ArUco tag, using the same camera and zoom level as calibration. I collected
        approximately 30–50 images at varying viewpoints and built a dataset in the same format
        as the Lego npz file.
      </p>

      <h3>Training Setup and Hyperparameters (Deliverable: Discussion)</h3>
      <p>
        Compared to the Lego scene, I made the following code and hyperparameter changes:
      </p>
      <ul>
        <li><code>near, far: 0.1 0.39182572066783905</code> </li>
        <li><strong><code>
          nerf = SimpleNeRF(num_freqs=10, hidden_dim=256).to(device)
          optimizer = optim.Adam(nerf.parameters(), lr=5e-4)
          criterion = nn.MSELoss()

          num_samples = 64
          batch_size = 10_000
          num_iters = 8000</code></li>
      </ul>

      <h3>Orbit GIF of My Object (Deliverable)</h3>
      <figure>
          <img src="source/gif_ref.png">
        </figure>
      <figure>
        <!-- Gif of your object orbit -->
        <img src="source/fruit.gif" alt="Orbiting novel views of my object">
        <figcaption>GIF of a camera circling my object, showing novel views.</figcaption>
      </figure>

      <h3>Training Loss Curve (Deliverable)</h3>
      <figure>
        <img src="source/fruit_plot.png" alt="Training loss over iterations for my object NeRF">
        <figcaption>Training loss vs. iterations for my object NeRF.</figcaption>
      </figure>
    </section>

  </div>
</body>
</html>
